{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSmSi6tymBzh"
      },
      "source": [
        "### **Comparing pruning methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmZ_hpDQmQuR"
      },
      "source": [
        "Investigation of weight and neuron pruning schemes of neural networks.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Z_WON1mLvh"
      },
      "source": [
        "## Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_ijymwNmcMp"
      },
      "source": [
        "import functools\r\n",
        "import time\r\n",
        "\r\n",
        "from tensorflow.keras import (\r\n",
        "\tactivations,\r\n",
        "\tdatasets,\r\n",
        "\tinitializers,\r\n",
        "\tlayers,\r\n",
        "\tlosses,\r\n",
        "\tmodels,\r\n",
        "\toptimizers,\r\n",
        "\tutils,\r\n",
        ")\r\n",
        "\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44YBw-LBmfzo"
      },
      "source": [
        "## Definition of constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCp47lGAmpIZ"
      },
      "source": [
        "if already trained (ie file named \"model.h5\" exists) change variable TRAIN to False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM4FpN5-mkho"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "PRUNING_FRACTIONS = (0, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.97, 0.99)\r\n",
        "LAYER_SIZES = (1000, 1000, 500, 200)  # Layer sizes without the output one.\r\n",
        "N_EPOCHS = 4\r\n",
        "N_TRIALS = 5  # Number of trials in inference time measurement.\r\n",
        "TRAIN = True"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkdW7uqvm0pK"
      },
      "source": [
        "## Main Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GoW-bK7m5WP"
      },
      "source": [
        "class Sparse(layers.Layer):\r\n",
        "\t\"\"\"Sparse layer without bias.\r\n",
        "\r\n",
        "\tWeight matrix (attribute kernel_t) is stored in transposed form, as\r\n",
        "\ttf.sparse.matmul is sparse x dense, not the other way around.\r\n",
        "\t\r\n",
        "\tArgs:\r\n",
        "\t\toutput_dim (int): Output dimensionality.\r\n",
        "\t\tn_nonzero (int): Number of non-zero entries in the weight matrix.\r\n",
        "\t\tactivation: Activation function.\r\n",
        "\t\tindices_initializer: Initializer for the indices of the transposed\r\n",
        "\t\t\tweight matrix - int array of shape (n_nonzero, 2). Should be in\r\n",
        "\t\t\tlexicographical order.\r\n",
        "\t\tvalues_initializer: Initializer for the nonzero entries - float array\r\n",
        "\t\t\tof shape (n_nonzero,). Should be in the same order as the indices.\r\n",
        "\t\"\"\"\r\n",
        "\r\n",
        "\tdef __init__(\r\n",
        "\t\tself,\r\n",
        "\t\toutput_dim,\r\n",
        "\t\tn_nonzero,\r\n",
        "\t\tactivation,\r\n",
        "\t\tindices_initializer,\r\n",
        "\t\tvalues_initializer,\r\n",
        "\t\t**kwargs\r\n",
        "\t):\r\n",
        "\t\tself.output_dim = output_dim\r\n",
        "\t\tself.n_nonzero = n_nonzero\r\n",
        "\t\tself.activation = activations.get(activation)\r\n",
        "\t\tself.indices_initializer = initializers.get(indices_initializer)\r\n",
        "\t\tself.values_initializer = initializers.get(values_initializer)\r\n",
        "\t\tsuper().__init__(**kwargs)\r\n",
        "\r\n",
        "\tdef build(self, input_shape):\r\n",
        "\t\t(_, input_dim) = input_shape\r\n",
        "\t\t# Indices of nonzero entries.\r\n",
        "\t\tself.indices = self.add_weight(\r\n",
        "\t\t\tname=\"indices\",\r\n",
        "\t\t\tshape=(self.n_nonzero, 2),\r\n",
        "\t\t\tdtype=tf.int64,\r\n",
        "\t\t\tinitializer=self.indices_initializer,\r\n",
        "\t\t\ttrainable=False,\r\n",
        "\t\t)\r\n",
        "\t\t# Values of nonzero entries.\r\n",
        "\t\tself.values = self.add_weight(\r\n",
        "\t\t\tname=\"values\",\r\n",
        "\t\t\tshape=(self.n_nonzero,),\r\n",
        "\t\t\tinitializer=self.values_initializer,\r\n",
        "\t\t\ttrainable=True,\r\n",
        "\t\t)\r\n",
        "\t\t# Sparse weight tensor (transposed).\r\n",
        "\t\tself.kernel_t = tf.SparseTensor(\r\n",
        "\t\t\tself.indices, self.values, dense_shape=(self.output_dim, input_dim)\r\n",
        "\t\t)\r\n",
        "\t\tsuper().build(input_shape)\r\n",
        "\r\n",
        "\tdef call(self, inputs):\r\n",
        "\t\t# TensorFlow supports only sparse-to-dense multiplication and we want to\r\n",
        "\t\t# do dense-to-sparse (inputs * kernel). We do this according to\r\n",
        "\t\t# B * A = (A^T * B^T)^T.\r\n",
        "\t\toutput = tf.transpose(tf.sparse.sparse_dense_matmul(self.kernel_t, tf.transpose(inputs)))\r\n",
        "\t\tif self.activation:\r\n",
        "\t\t\toutput = self.activation(output)\r\n",
        "\t\treturn output\r\n",
        "\r\n",
        "\tdef compute_output_shape(self, input_shape):\r\n",
        "\t\t(_, input_dim) = input_shape\r\n",
        "\t\treturn (input_dim, self.output_dim)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Tn3A8cnBtI"
      },
      "source": [
        "## Working on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G2xLxZrnFms"
      },
      "source": [
        "def preprocess_dataset(x, y):\r\n",
        "\t\"\"\"Performs preprocessing of the dataset before feeding it to the NN.\"\"\"\r\n",
        "\t# Add the channel dimension.\r\n",
        "\tx = np.expand_dims(x, axis=-1)\r\n",
        "\t# Rescale to [-1, 1].\r\n",
        "\tx = x.astype(np.float32)\r\n",
        "\tx /= 128\r\n",
        "\tx -= 0.5\r\n",
        "\t# One-hot encode the labels.\r\n",
        "\ty = utils.to_categorical(y)\r\n",
        "\treturn (x, y)\r\n",
        "\r\n",
        "\r\n",
        "def dense_kwargs(weight_matrix):\r\n",
        "\t\"\"\"Defines kwargs needed for the Dense layer to initialize its weights.\"\"\"\r\n",
        "\treturn {\"kernel_initializer\": initializers.Constant(weight_matrix)}\r\n",
        "\r\n",
        "\r\n",
        "def sparse_kwargs(weight_matrix):\r\n",
        "\t\"\"\"Defines kwargs needed for the Sparse layer to initialize its weights.\"\"\"\r\n",
        "\tweight_matrix_t = np.transpose(weight_matrix)\r\n",
        "\tnonzero_arrays = np.nonzero(weight_matrix_t)\r\n",
        "\tindices = np.transpose(nonzero_arrays)\r\n",
        "\tvalues = weight_matrix_t[nonzero_arrays]\r\n",
        "\treturn {\r\n",
        "\t\t\"n_nonzero\": len(values),\r\n",
        "\t\t\"indices_initializer\": initializers.Constant(indices),\r\n",
        "\t\t\"values_initializer\": initializers.Constant(values),\r\n",
        "\t}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvPOXGznNR9"
      },
      "source": [
        "## Auxilary function \r\n",
        "Used to create a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfb7zFK_nTI7"
      },
      "source": [
        "def create_model(layer_sizes, n_classes, layer_fn, layer_kwargs_fn, weights=None):\r\n",
        "\t\"\"\"Creates the model, optionally initializing its weights.\r\n",
        "\r\n",
        "\tArgs:\r\n",
        "\t\tlayer_sizes (list): Numbers of neurons in consecutive layers, excluding\r\n",
        "\t\t\tthe output layer.\r\n",
        "\t\tn_classes (int): Number of classification classes.\r\n",
        "\t\tlayer_fn: Factory function for hidden layers. Should accept at least\r\n",
        "\t\t\toutput_dim, activation and everything returned from layer_kwargs_fn.\r\n",
        "\t\tlayer_kwargs_fn: Function weights -> kwargs defining kwargs needed\r\n",
        "\t\t\tfor the given layer_fn to initialize it with the given weights.\r\n",
        "\t\tweights (list or None): List of weight matrices to initialize the model\r\n",
        "\t\t\twith. Defaults to None - use random initialization.\r\n",
        "\r\n",
        "\tReturns:\r\n",
        "\t\tA compiled model.\r\n",
        "\t\"\"\"\r\n",
        "\tmodel = models.Sequential()\r\n",
        "\tmodel.add(layers.Flatten())\r\n",
        "\r\n",
        "\tdef layer_kwargs(layer_kwargs_fn, i):\r\n",
        "\t\tif weights is not None:\r\n",
        "\t\t\treturn layer_kwargs_fn(weights[i])\r\n",
        "\t\telse:\r\n",
        "\t\t\treturn {}\r\n",
        "\r\n",
        "\t# Build the hidden layers using the provided layer_fn.\r\n",
        "\tfor (i, layer_size) in enumerate(layer_sizes):\r\n",
        "\t\tmodel.add(\r\n",
        "\t\t\tlayer_fn(layer_size, activation=\"relu\", **layer_kwargs(layer_kwargs_fn, i))\r\n",
        "\t\t)\r\n",
        "\t# We don't prune the output layer so it's always Dense.\r\n",
        "\tmodel.add(\r\n",
        "\t\tlayers.Dense(\r\n",
        "\t\t\tn_classes,\r\n",
        "\t\t\tactivation=\"softmax\",\r\n",
        "\t\t\tuse_bias=False,\r\n",
        "\t\t\t**layer_kwargs(dense_kwargs, len(layer_sizes))\r\n",
        "\t\t)\r\n",
        "\t)\r\n",
        "\r\n",
        "\tmodel.compile(\r\n",
        "\t\tloss=losses.categorical_crossentropy,\r\n",
        "\t\toptimizer=optimizers.Adam(),\r\n",
        "\t\tmetrics=[\"accuracy\"],\r\n",
        "\t)\r\n",
        "\r\n",
        "\treturn model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__DdIWDUneiw"
      },
      "source": [
        "## Pruning methods\r\n",
        "These are the two pruning methods :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtyL4n1NnlEH"
      },
      "source": [
        "def prune_weights(model, fraction):\r\n",
        "\t\"\"\"Prunes a fraction of model weights.\"\"\"\r\n",
        "\tweights = model.get_weights()\r\n",
        "\r\n",
        "\tdef prune_weight_matrix(weight_matrix):\r\n",
        "\t\t# Copy the weights so we don't modify the original network.\r\n",
        "\t\tweight_matrix = np.copy(weight_matrix)\r\n",
        "\t\tflat_weight_matrix = np.reshape(weight_matrix, (-1,))\r\n",
        "\t\tkth = int(len(flat_weight_matrix) * fraction)\r\n",
        "\t\t# Determine the k least relevant weights using np.argpartition.\r\n",
        "\t\tindices = np.argpartition(np.abs(flat_weight_matrix), kth)\r\n",
        "\t\t# Prune them.\r\n",
        "\t\tflat_weight_matrix[indices[:kth]] = 0\r\n",
        "\t\tweight_matrix = np.reshape(flat_weight_matrix, weight_matrix.shape)\r\n",
        "\t\treturn weight_matrix\r\n",
        "\r\n",
        "\tweights[:-1] = list(map(prune_weight_matrix, weights[:-1]))\r\n",
        "\r\n",
        "\t(_, n_classes) = weights[-1].shape\r\n",
        "\t# Create a pruned model.\r\n",
        "\treturn create_model(\r\n",
        "\t\tLAYER_SIZES,\r\n",
        "\t\tn_classes,\r\n",
        "\t\tlayer_fn=Sparse,\r\n",
        "\t\tlayer_kwargs_fn=sparse_kwargs,\r\n",
        "\t\tweights=weights,\r\n",
        "\t)\r\n",
        "\r\n",
        "\r\n",
        "def prune_neurons(model, fraction):\r\n",
        "\t\"\"\"Prunes a fraction of model neurons.\"\"\"\r\n",
        "\tweights = model.get_weights()\r\n",
        "\r\n",
        "\tdef nonzero_indices(weight_matrix):\r\n",
        "\t\tneuron_norms = np.linalg.norm(weight_matrix, axis=0)\r\n",
        "\t\tkth = int(len(neuron_norms) * fraction)\r\n",
        "\t\t# Determine the k least relevant neurons using np.argpartition.\r\n",
        "\t\treturn np.argpartition(neuron_norms, kth)[kth:]\r\n",
        "\r\n",
        "\t(n_inputs, _) = weights[0].shape\r\n",
        "\t# Remember which neurons we left in the last layer - we'll need to know that\r\n",
        "\t# to prune the next one. At first it's all of the inputs as we don't prune\r\n",
        "\t# them.\r\n",
        "\tlast_indices = np.arange(n_inputs)\r\n",
        "\tlayer_sizes = []\r\n",
        "\tfor (i, weight_matrix) in enumerate(weights[:-1]):\r\n",
        "\t\tindices = nonzero_indices(weight_matrix)\r\n",
        "\t\tlayer_sizes.append(len(indices))\r\n",
        "\t\t# Take a subset of both rows and columns.\r\n",
        "\t\tweights[i] = weight_matrix[last_indices, :][:, indices]\r\n",
        "\t\tlast_indices = indices\r\n",
        "\t# Take a subset of rows for the last layer - we don't prune the outputs.\r\n",
        "\tweights[-1] = weights[-1][last_indices, :]\r\n",
        "\r\n",
        "\t(_, n_classes) = weights[-1].shape\r\n",
        "\t# Create a pruned model.\r\n",
        "\treturn create_model(\r\n",
        "\t\tlayer_sizes,\r\n",
        "\t\tn_classes,\r\n",
        "\t\tlayer_fn=functools.partial(layers.Dense, use_bias=False),\r\n",
        "\t\tlayer_kwargs_fn=dense_kwargs,\r\n",
        "\t\tweights=weights,\r\n",
        "\t)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ads54QfMnouz"
      },
      "source": [
        "## Evaluation of a pruning method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hvGlJUinwPn"
      },
      "source": [
        "def evaluate_fraction(pruning_fn, model, dataset, fraction):\r\n",
        "\t\"\"\"Evaluates a pruning fraction on a given model.\r\n",
        "\r\n",
        "\tArgs:\r\n",
        "\t\tpruning_fn: Function (model, fraction) -> model.\r\n",
        "\t\tmodel: Keras model.\r\n",
        "\t\tdataset: Pair (inputs, labels).\r\n",
        "\t\tfraction (float): A fraction of the model to prune.\r\n",
        "\r\n",
        "\tReturns:\r\n",
        "\t\tPair (accuracy, inference time).\r\n",
        "\t\"\"\"\r\n",
        "\t# Run the model on CPU to avoid copying between CPU and GPU that would\r\n",
        "\t# dominate the time cost.\r\n",
        "\twith tf.device(\"/cpu:0\"):\r\n",
        "\t\tmodel = pruning_fn(model, fraction)\r\n",
        "\t\t# Measure accuracy on the test set.\r\n",
        "\t\t(_, accuracy) = model.evaluate(*dataset)\r\n",
        "\r\n",
        "\t\t(inputs, _) = dataset\r\n",
        "\t\tstart_time = time.time()\r\n",
        "\t\t# Measure inference time by repeatedly running prediction on the test\r\n",
        "\t\t# set. Feed the whole dataset at once to remove the impact of batching.\r\n",
        "\t\tfor _ in range(N_TRIALS):\r\n",
        "\t\t\tmodel.predict_on_batch(inputs)\r\n",
        "\t\ttrial_time = (time.time() - start_time) / N_TRIALS\r\n",
        "\r\n",
        "\t\treturn (accuracy, trial_time)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEY2USTanzBj"
      },
      "source": [
        "## **Main()**\r\n",
        "Evaluates the two pruning methods and plots the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ1cEKgPh3GP",
        "outputId": "6c718e81-3805-43c7-966f-93f65081a93d"
      },
      "source": [
        "def main():\n",
        "\t# Load and preprocess the data.\n",
        "\t((x_train, y_train), (x_test, y_test)) = (\n",
        "\t\tpreprocess_dataset(x, y) for (x, y) in datasets.mnist.load_data()\n",
        "\t)\n",
        "\t(_, n_classes) = y_train.shape\n",
        "\n",
        "\t# Create and fit the original model.\n",
        "\tif TRAIN:\n",
        "\t\tmodel = create_model(\n",
        "\t\t\tLAYER_SIZES,\n",
        "\t\t\tn_classes,\n",
        "\t\t\tlayer_fn=functools.partial(layers.Dense, use_bias=False),\n",
        "\t\t\tlayer_kwargs_fn=dense_kwargs,\n",
        "\t\t)\n",
        "\t\tmodel.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)\n",
        "\t\tmodel.save('model.h5')\n",
        "\telse:\n",
        "\t\tmodel = models.load_model('model.h5')\n",
        "\t\n",
        "\t# Evaluate the pruning methods.\n",
        "\tpruning_methods = [(\"weight\", prune_weights), (\"neuron\", prune_neurons)]\n",
        "\tfor (method_name, pruning_fn) in pruning_methods:\n",
        "\t\t# Compute the evaluation curves.\n",
        "\t\t(accuracy_curve, time_curve) = zip(\n",
        "\t\t\t*[\n",
        "\t\t\t\tevaluate_fraction(pruning_fn, model, (x_test, y_test), fraction)\n",
        "\t\t\t\tfor fraction in PRUNING_FRACTIONS\n",
        "\t\t\t]\n",
        "\t\t)\n",
        "\t\t# Plot them.\n",
        "\t\tfor (subplot, curve) in [(211, accuracy_curve), (212, time_curve)]:\n",
        "\t\t\tplt.subplot(subplot)\n",
        "\t\t\tplt.plot(PRUNING_FRACTIONS, curve, label=method_name)\n",
        "\n",
        "\t\t\tif method_name == \"neuron\":\n",
        "\t\t\t\t# The first evaluated model in neuron pruning is the unpruned\n",
        "\t\t\t\t# one. Use it as a baseline.\n",
        "\t\t\t\t(baseline, *_) = curve\n",
        "\t\t\t\tplt.plot((0, 1), (baseline, baseline), \"--\", label=\"unpruned\")\n",
        "\n",
        "\t# Add some labels to the plots.\n",
        "\tfor (subplot, y_label) in [(211, \"accuracy\"), (212, \"inference time\")]:\n",
        "\t\tplt.subplot(subplot)\n",
        "\t\tplt.xlabel(\"pruning fraction\")\n",
        "\t\tplt.ylabel(y_label)\n",
        "\t\tplt.legend()\n",
        "\tplt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tmain()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 6s 20ms/step - loss: 0.0964 - accuracy: 0.9720\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}